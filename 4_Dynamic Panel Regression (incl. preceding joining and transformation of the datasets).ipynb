{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Panel regressions\n",
    "\n",
    "## Section 1: Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump\n",
    "import os\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define sentiment measure (move cell to select)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio filtered sentiment\n",
    "sent_choice_pos=\"Sent_ratio_filt_\"\n",
    "sent_choice_neg=(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive confidence in sentiment \n",
    "sent_choice_pos=\"Sent_conf_pos_\"\n",
    "sent_choice_neg=(\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Sentiment Filtered\n",
    "sent_choice_pos=\"Sent_avg_filt_\"\n",
    "sent_choice_neg=(\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive Sentiment Filtered\n",
    "sent_choice_pos=\"Sent_pos_filt_\"\n",
    "sent_choice_neg=(\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average confidence in sentiment \n",
    "sent_choice_pos=\"Sent_avg_conf_neg_\"\n",
    "sent_choice_neg=(\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative confidence in sentiment \n",
    "sent_choice_pos=\"Sent_conf_pos_\"\n",
    "sent_choice_neg=(\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio sentiment\n",
    "sent_choice_pos=\"Sent_ratio_\"\n",
    "sent_choice_neg=(\"Sent_ratio_filt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative confidence in sentiment \n",
    "sent_choice_pos=\"Sent_conf_neg_\"\n",
    "sent_choice_neg=(\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which sentiment score should be loaded\n",
    "\n",
    "# Negative Sentiment\n",
    "sent_choice_pos=\"Sent_neg_\"\n",
    "sent_choice_neg=(\"Sent_neg_filt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Sentiment Filtered\n",
    "sent_choice_pos=\"Sent_neg_filt_\"\n",
    "sent_choice_neg=(\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Sentiment \n",
    "sent_choice_pos=\"Sent_avg_\"\n",
    "sent_choice_neg=(\"Sent_avg_filt_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average confidence in sentiment \n",
    "sent_choice_pos=\"News_vol_\"\n",
    "sent_choice_neg=(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average confidence in sentiment \n",
    "sent_choice_pos=\"Sent_avg_conf_\"\n",
    "sent_choice_neg=(\"Sent_avg_conf_n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio sentiment confidence\n",
    "sent_choice_pos=\"Sent_ratio_conf_\"\n",
    "sent_choice_neg=(\"Sent_ratio_filt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute difference in sentiment confidence\n",
    "sent_choice_pos=\"Sent_conf_abs_\"\n",
    "sent_choice_neg=(\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Transform prices to returns\n",
    "\n",
    "### Section 1.1: Load data, exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices = pd.read_excel(\"../01_Data/01_Eikon/2_Prices/2_Full Stock Prices data/2_WRDS_SP 500 Full stock price.xlsx\", index_col=\"Date\")\n",
    "df_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some companies prices \n",
    "plt.subplot(311)\n",
    "df_prices['AAPL.O'].plot(figsize=(16, 6), legend=True)\n",
    "plt.subplot(312)\n",
    "df_prices['PG'].plot(figsize=(16, 6), legend=True)\n",
    "plt.subplot(313)\n",
    "df_prices['.SPX'].plot(figsize=(16, 6), legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate returns, fills Nan with prior price by default\n",
    "df_returns=df_prices.pct_change().dropna(how=\"all\")\n",
    "\n",
    "# Store to excel \n",
    "#df_returns.to_excel(\"../01_Data/10_Modelling/50_Daily_firm_specific_returns.xlsx\")\n",
    "\n",
    "df_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the unconditional mean of SPX returns in % \n",
    "print(df_returns[df_returns.index <\"2019-09-01\"]['.SPX'].mean()*100)\n",
    "mean_in_bps=df_returns[df_returns.index <\"2019-09-01\"]['.SPX'].mean()*10000\n",
    "print(mean_in_bps)\n",
    "print(1.8/mean_in_bps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the unconditional mean of SPX returns in % \n",
    "print(df_returns[(df_returns.index >\"2019-12-31\") & (df_returns.index <\"2020-03-01\")]['.SPX'].mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the unconditional mean of SPX returns in % \n",
    "print(df_returns[(df_returns.index >\"2020-02-29\")]['.SPX'].mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe that contains excess returns over the S&P 500 (market returns)\n",
    "df_excess_returns = pd.DataFrame()\n",
    "for x in df_returns.columns:\n",
    "    df_excess_returns[x] = df_returns[x] - df_returns[\".SPX\"]\n",
    "\n",
    "# Drop  column including S&P returns    \n",
    "df_excess_returns.drop([\".SPX\"], axis=1, inplace=True)\n",
    "\n",
    "df_excess_returns\n",
    "\n",
    "# Store to excel \n",
    "#df_excess_returns.to_excel(\"../01_Data/10_Modelling/50_Daily_firm_specific_excess_returns.xlsx\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check results with unadjusted returns\n",
    "\n",
    "# Create a new dataframe that contains excess returns over the S&P 500 (market returns)\n",
    "df_excess_returns = pd.DataFrame()\n",
    "for x in df_returns.columns:\n",
    "    df_excess_returns[x] = df_returns[x] \n",
    "\n",
    "# Drop  column including S&P returns    \n",
    "df_excess_returns.drop([\".SPX\"], axis=1, inplace=True)\n",
    "\n",
    "df_excess_returns\n",
    "\n",
    "# Store to excel \n",
    "#df_excess_returns.to_excel(\"../01_Data/10_Modelling/50_Daily_firm_specific_excess_returns.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data transformation \n",
    "\n",
    "### Section 2.1: Transform return data to panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=df_excess_returns.reset_index()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform return data to panel data\n",
    "df_ex_ret_transf = df_excess_returns.reset_index().melt(id_vars='Date',var_name = 'Company', value_name = 'Excess_returns')\n",
    "df_ex_ret_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows for companies with NaN Excess_returns (days not in index)\n",
    "df_ex_ret_transf= df_ex_ret_transf[df_ex_ret_transf['Excess_returns'].notna()]\n",
    "df_ex_ret_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Company and date as multiindex\n",
    "df_ex_ret_transf = df_ex_ret_transf.set_index(['Company', 'Date'])\n",
    "df_ex_ret_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged returns for each companies' returns\n",
    "for i in range(1,6):\n",
    "    df_ex_ret_transf['ER_L'+str(i)] = df_ex_ret_transf.groupby(level=0)['Excess_returns'].shift(i)\n",
    "df_ex_ret_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex_ret_transf=df_ex_ret_transf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to the right format\n",
    "df_ex_ret_transf.Date=pd.to_datetime(df_ex_ret_transf[\"Date\"]).dt.date\n",
    "df_ex_ret_transf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: Transform sentiment data to panel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load sentiment scores\n",
    "df_sp500_scores=pd.read_excel(\"../01_Data/10_Modelling/32_word2vec_Sentiment Analysis_Semeval_Daily_firm_specific_sentiment_scores.xlsx\",\\\n",
    "                              usecols=lambda x: x.startswith((\"Date\",sent_choice_pos)) and not x.startswith(sent_choice_neg))\n",
    "df_sp500_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform sentiment data to panel data\n",
    "df_sp500_scores = df_sp500_scores.melt(id_vars='Date',var_name = 'Company', value_name = sent_choice_pos)\n",
    "df_sp500_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Sentiment Descr from company label\n",
    "df_sp500_scores[\"Company\"]=df_sp500_scores['Company'].str.replace(sent_choice_pos, '')\n",
    "df_sp500_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp500_scores.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp500_scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sp500_scores[sent_choice_pos].plot(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identify highly negative score \n",
    "#df_sp500_scores[df_sp500_scores[sent_choice_pos]>100][\"Companies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# consider winsorizing the data\n",
    "\n",
    "\"\"\"from scipy.stats.mstats import winsorize\n",
    "\n",
    "# test winsorising the top 5 % \n",
    "df_main[sent_choice_pos]= winsorize(df_main[sent_choice_pos], limits=[0.0001, 0.1],nan_policy=\"omit\")\n",
    "df_main[sent_choice_pos].plot(figsize=(15,5))\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# fit scaler\n",
    "scaler = StandardScaler()\n",
    "#scaler.fit(df_sp500_scores.Sent_conf_neg_)\n",
    "\n",
    "# scale data (to obtain a mean of 0 and a stdev. of 1 --> easier to analyse regression results)\n",
    "# then the impact of a coef +1 measures the impact of a one stdev. increase\n",
    "df_sp500_scores.iloc[:,2]=scaler.fit_transform(df_sp500_scores.iloc[:,2].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values (days without news)\n",
    "df_sp500_scores=df_sp500_scores.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to the right format\n",
    "df_sp500_scores.Date=pd.to_datetime(df_sp500_scores[\"Date\"]).dt.date\n",
    "df_sp500_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Laggs through weekends\n",
    "\n",
    "# Set Company and date as multiindex\n",
    "df_sp500_scores = df_sp500_scores.set_index(['Company', 'Date'])\n",
    "df_sp500_scores\n",
    "\n",
    "# Create lagged returns for each companies' returns\n",
    "for i in range(1,6):\n",
    "    df_sp500_scores[sent_choice_pos+'_L'+str(i)] = df_sp500_scores.groupby(level=0)[sent_choice_pos].shift(i)\n",
    "df_sp500_scores\n",
    "\n",
    "df_sp500_scores.to_excel(\"test_score.xlsx\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3: Merge Panel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge dataframes (left outer)\n",
    "#df_main = pd.merge(df_sp500_scores, df_excess_returns, left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "# merge dataframes (left - only keep trading days)\n",
    "df_main = pd.merge(df_ex_ret_transf,df_sp500_scores, on=[\"Date\",\"Company\"], how=\"left\")\n",
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all companies except the ones analysed by Ahmad\n",
    "list_comp=[\"AAPL\",\"BA\",\"CVX\",\"F\", \"GE\",\"HD\",\"HPE\",\"IBM\",\"INTC\",\"JNJ\",\"MRK\",\"MSFT\",\"PFE\",\"VZ\", \"WMT\"]\n",
    "#df_main=df_main[df_main.Company.isin(list_comp)]\n",
    "#dell not constituation anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Company and date as multiindex\n",
    "df_main = df_main.set_index(['Company', 'Date'])\n",
    "df_main\n",
    "\n",
    "# Create lagged returns for each companies' returns\n",
    "for i in range(1,6):\n",
    "    df_main[sent_choice_pos+'L_'+str(i)] = df_main.groupby(level=0)[sent_choice_pos].shift(i)\n",
    "df_main\n",
    "\n",
    "df_main = df_main.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check that data is stacked\n",
    "df_main.iloc[33:340]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_main=df_main[df_main.Company!=\"MCO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store to excel\n",
    "#df_main.to_excel(\"../01_Data/10_Modelling/50_Stata_Panel_Dataframe.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values linearly\n",
    "#df.interpolate(method=\"linear\", axis=0).ffill().bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.4: Set up control variables / dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_days={0:\"Mon\",\n",
    "          1:\"Tue\",\n",
    "          2:\"Wed\",\n",
    "          3:\"Thu\",\n",
    "          4:\"Fri\",\n",
    "          5:\"Sat\",\n",
    "          6:\"Sun\"}\n",
    "\n",
    "\n",
    "# Insert controll variable for the day of the week (0=Monday, 6=Sunday)\n",
    "df_main[\"Weekday\"]=pd.to_datetime(df_main.Date).dt.dayofweek.map(dict_days)\n",
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy for monday\n",
    "df_main[\"Monday\"]=[1 if x==\"Mon\" else 0 for x in df_main[\"Weekday\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"# Insert control variable for January (Tetlock 2007), \n",
    "# outcommented as no January in training set \n",
    "df_main[\"January\"]=[1 if x==1 else 0 for x in pd.to_datetime(df_main.Date).dt.month]\n",
    "df_main\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove weekday column\n",
    "df_main.drop(columns=[\"Weekday\"],inplace=True)\n",
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Company Dummies (FE Regression)\n",
    "df_main[\"Companies\"]=df_main[\"Company\"]\n",
    "df_main=pd.get_dummies(df_main,columns=[\"Company\"], drop_first=True)\n",
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set index back to Date\n",
    "df_main= df_main.set_index(\"Date\")\n",
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "#df_main.to_csv(\"../01_Data/10_Modelling/50_Stata_Panel_Dataframe_incl_dummies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Dynamic Panel regressions\n",
    "\n",
    "#### Section 3.1: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(df_main, test_size=0.2, random_state=7, shuffle=False)\n",
    "print(f\"{train_set.shape[0]} train and {test_set.shape[0]} test instances\")\n",
    "\"\"\"\n",
    "# use 80% / 20% roughly -> use time series split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "train_set=df_main[df_main.index < \"2019-10-01\"]\n",
    "test_set_pre_corona=df_main[(df_main.index > \"2019-09-30\")&(df_main.index < '2020-03-01')]\n",
    "test_set_corona=df_main[df_main.index > '2020-03-01']\n",
    "test_set_full=df_main[df_main.index > '2019-09-30']\n",
    "\n",
    "\n",
    "#test_set_pre_corona=df_main[((df_main.index > \"2019-09-30\") & (df_main.index < \"2020-01-01\"))| \\\n",
    "#                            ((df_main.index > '2020-01-31')& (df_main.index < \"2020-03-01\"))]\n",
    "\n",
    "#test_set_pre_corona=df_main[(df_main.index > \"2019-09-30\")&(df_main.index < '2020-01-01')]\n",
    "#test_set_corona=df_main[df_main.index > '2019-12-31']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Split by pre corona and post corona\n",
    "# sort by date to ensure that all companies are included in train and test (no need, already correct)\n",
    "train_set=df_main.loc['2019-03-01':'2019-09-30']\n",
    "test_set_pre_corona=df_main.loc['2019-10-01':'2020-02-29']\n",
    "test_set_corona=df_main.loc['2020-03-01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set.sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test if testset is correctly specified\n",
    "print(train_set.equals(df_main[df_main.index < \"2019-10-01\"]))\n",
    "# test for pre corona testset\n",
    "print(test_set_pre_corona.equals(df_main[(df_main.index > \"2019-09-30\")&(df_main.index < '2020-03-01')]))\n",
    "# test for corona testset\n",
    "print(test_set_corona.equals(df_main[df_main.index > '2020-03-01']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to excel to check if datasets are filtered correctly\n",
    "#train_set[\"Companies\"].to_excel(\"test.xlsx\")\n",
    "#test_set_pre_corona[\"Companies\"].to_excel(\"test2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.2  Data cleaning and transformation\n",
    "\n",
    "To build a VAR model, the data needs to be stationary.\n",
    "To do so, we will firstly test for that using the Augmented Dickey-Fuller (ADF) test and the KPSS (Kwiatkowski-Phillips-Schmidt-Shin) tests.\n",
    "\n",
    "The test will only be applied on the training data and if necessary any transformations will be applied to both sets.\n",
    "\n",
    "First, we will test the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# replace missing data by 0\n",
    "#train_set=train_set.fillna(0)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns\n",
    "list_columns=[\"Excess_returns\",sent_choice_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "# implement stationarity check for each timeseries of the selected sentiment measure\n",
    "\n",
    "# transform the table back to a company per column\n",
    "train_set_test=train_set.pivot(index=train_set.index,columns=\"Companies\")[sent_choice_pos]\n",
    "train_set_test\n",
    "\n",
    "list_results=[]\n",
    "\n",
    "for x in train_set_test.columns:\n",
    "    adf_pval = adfuller(train_set_test.fillna(0)[x])[1]\n",
    "   # print(x)\n",
    "  #  print(f\"ADF, p-value: {adf_pval}\")\n",
    "    kpss_stat, kpss_pval, lags, crit_vals = kpss(train_set_test.fillna(0)[x])\n",
    "   # print(f\"KPSS, p-value: {kpss_pval}\")\n",
    "    \n",
    "    list_results.append(\n",
    "        {\n",
    "            'Company': x,\n",
    "            'ADF': adf_pval,\n",
    "            'KPSS':  kpss_pval\n",
    "        }\n",
    "    )\n",
    "    \n",
    "df_results=pd.DataFrame(list_results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create stationarity flag\n",
    "def cond_check(df):\n",
    "    if (df.ADF < 0.05) and (df.KPSS >0.05):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# apply function\n",
    "df_results[\"stationary_flag\"]=df_results.apply(cond_check,axis=1)\n",
    "\n",
    "# print result\n",
    "print(\"Non-stationary Sentiment Timeseries: \" + df_results[df_results.stationary_flag==0][\"stationary_flag\"].count().astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show companies that fail stationary criterium\n",
    "list_drop=[]\n",
    "list_drop=df_results[df_results[\"stationary_flag\"]==0][\"Company\"]\n",
    "list_drop[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop companies that failed the stationary criterion from dataframe\n",
    "train_set=train_set[~train_set.Companies.isin(list_drop)]\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "# implement stationarity check for each timeseries of the selected sentiment measure\n",
    "\n",
    "# transform the table back to a company per column\n",
    "train_set_test=train_set.pivot(index=train_set.index,columns=\"Companies\")[\"Excess_returns\"]\n",
    "train_set_test\n",
    "\n",
    "list_results=[]\n",
    "\n",
    "for x in train_set_test.columns:\n",
    "    adf_pval = adfuller(train_set_test.fillna(0)[x])[1]\n",
    "   # print(x)\n",
    "  #  print(f\"ADF, p-value: {adf_pval}\")\n",
    "    kpss_stat, kpss_pval, lags, crit_vals = kpss(train_set_test.fillna(0)[x])\n",
    "   # print(f\"KPSS, p-value: {kpss_pval}\")\n",
    "    \n",
    "    list_results.append(\n",
    "        {\n",
    "            'Company': x,\n",
    "            'ADF': adf_pval,\n",
    "            'KPSS':  kpss_pval\n",
    "        }\n",
    "    )\n",
    "    \n",
    "df_results=pd.DataFrame(list_results)\n",
    "#df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[\"stationary_flag\"]=df_results.apply(cond_check,axis=1)\n",
    "print(\"Non-stationary Return Timeseries: \" + df_results[df_results.stationary_flag==0][\"stationary_flag\"].count().astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show companies that fail stationary criterium\n",
    "list_drop=df_results[df_results[\"stationary_flag\"]==0][\"Company\"]\n",
    "list_drop[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns should be stationary. As they are already differenced. However, some time series are not. Thus, they will be dropped too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop companies that failed the stationary criterion from dataframe\n",
    "train_set=train_set[~train_set.Companies.isin(list_drop)]\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dropping of the companies that failed the stationary criterion is performed later \n",
    "# in the out of sample performance checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results.to_excel(\"1_Results/Stationarity/Stationarity_Test_\"+sent_choice_pos+\"_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set[sent_choice_pos].plot(figsize=(16, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stationarise for avg, abs only by log or differencing."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Try to stationarize using log \n",
    "train_set[sent_choice_pos]=train_set[sent_choice_pos].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "logs = np.log(train_set[sent_choice_pos]).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "adf_pval = adfuller(logs)[1]\n",
    "print(f\"ADF, p-value: {adf_pval}\")\n",
    "kpss_stat, kpss_pval, lags, crit_vals = kpss(logs)\n",
    "print(f\"KPSS, p-value: {kpss_pval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.2.1  Data Differencing\n",
    "Log transformation is not possible as 0 are included.\n",
    "Thus, **differencing** is used."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Set Company and date as multiindex\n",
    "train_diff=train_set.reset_index()\n",
    "train_diff = train_diff.set_index(['Companies', 'Date'])\n",
    "train_diff"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Take first difference by group for sentiment choice\n",
    "train_diff[sent_choice_pos]=train_diff.groupby(level=0)[sent_choice_pos].diff()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# confirm expected result\n",
    "train_diff.iloc[0:15,6:12]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create grouped differences  for each companies' sentiment for each lag\n",
    "for i in range(1,6):\n",
    "    train_diff[sent_choice_pos+'L_'+str(i)] = train_diff.groupby(level=0)[sent_choice_pos+'L_'+str(i)].diff()\n",
    "train_diff.iloc[0:15,6:12]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check for each timeseries if differencing resulted in a stationary timeseries\n",
    "\n",
    "\n",
    "## still need to implement but result should be ok \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_diff=train_diff.reset_index()\n",
    "train_diff=train_diff.set_index(['Date'])\n",
    "\n",
    "# drop companies from both datasets\n",
    "#train_diff = train_set.drop(['Companies'],axis=1)\n",
    "\n",
    "# replace train_set with train_diff\n",
    "train_set= train_diff"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_set.iloc[0:15,6:12]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# transform test data too\n",
    "# adjust to transform both test sets!\n",
    "\n",
    "# Set Company and date as multiindex\n",
    "test_diff_pre_corona=test_set_pre_corona.reset_index()\n",
    "test_diff_pre_corona = test_diff_pre_corona.set_index(['Companies', 'Date'])\n",
    "\n",
    "# Take first difference by group\n",
    "test_diff_pre_corona[sent_choice_pos]=test_diff_pre_corona.groupby(level=0)[sent_choice_pos].diff()\n",
    "\n",
    "# Create grouped differences  for each companies' returns\n",
    "for i in range(1,6):\n",
    "    test_diff_pre_corona[sent_choice_pos+'L_'+str(i)] = test_diff_pre_corona.groupby(level=0)[sent_choice_pos+'L_'+str(i)].diff()\n",
    "test_diff_pre_corona\n",
    "\n",
    "# drop rows that contain a missing value for returns\n",
    "test_diff_pre_corona.dropna(subset=[\"ER_L1\",\"ER_L2\",\"ER_L3\",\"ER_L4\",\"ER_L5\",\"Excess_returns\"],inplace=True)\n",
    "\n",
    "# replace nan by 0 for sentiment columns\n",
    "test_diff_pre_corona=test_diff_pre_corona.fillna(0)\n",
    "\n",
    "test_diff_pre_corona=test_diff_pre_corona.reset_index()\n",
    "test_diff_pre_corona = test_diff_pre_corona.set_index(['Date'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# transform test data too\n",
    "# adjust to transform both test sets!\n",
    "\n",
    "# Set Company and date as multiindex\n",
    "test_diff_corona=test_set_corona.reset_index()\n",
    "test_diff_corona = test_diff_corona.set_index(['Companies', 'Date'])\n",
    "\n",
    "# Take first difference by group\n",
    "test_diff_corona[sent_choice_pos]=test_diff_corona.groupby(level=0)[sent_choice_pos].diff()\n",
    "\n",
    "# Create grouped differences  for each companies' returns\n",
    "for i in range(1,6):\n",
    "    test_diff_corona[sent_choice_pos+'L_'+str(i)] = test_diff_corona.groupby(level=0)[sent_choice_pos+'L_'+str(i)].diff()\n",
    "test_diff_corona\n",
    "\n",
    "# drop rows that contain a missing value for returns\n",
    "test_diff_corona.dropna(subset=[\"ER_L1\",\"ER_L2\",\"ER_L3\",\"ER_L4\",\"ER_L5\",\"Excess_returns\"],inplace=True)\n",
    "\n",
    "# replace nan by 0 for sentiment columns\n",
    "test_diff_corona=test_diff_corona.fillna(0)\n",
    "\n",
    "test_diff_corona=test_diff_corona.reset_index()\n",
    "test_diff_corona = test_diff_corona.set_index(['Date'])\n",
    "\n",
    "# drop companies from both datasets\n",
    "test_diff_pre_corona = test_diff_pre_corona.drop(['Companies'],axis=1)\n",
    "test_diff_corona = test_diff_corona.drop(['Companies'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set['Companies'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both tests indicate that all the three series become stationary after first-differencing: the ADF test rejects the null of unit root, and the KPSS test fails to reject the null of stationarity, at the 0.05 significance level. Thus, there is no need to stationarize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop company column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop companies from training datasets\n",
    "train_set = train_set.drop(['Companies'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.3 Determine the order of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from statsmodels.tsa.vector_ar.var_model import VAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `select_order` method to determine the best order: taking the maximum number of lags, the method will build VAR models for each number of lags and output the values of IC for each.\n",
    "\n",
    "The optimal number of lags are indicated with an asterisk in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = VAR(train_set[list_columns]).select_order(maxlags=5)\n",
    "#results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All information criteria suggest 5 lags. Let's go for the lag of 5.\n",
    "\n",
    "### 3.4 Estimate a Dynamic Panel Regression\n",
    "\n",
    "### 3.4.1 Dynamic Panel Regression for Excess Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# drop rows that contain a missing value for returns\n",
    "train_set.dropna(subset=[\"ER_L1\",\"ER_L2\",\"ER_L3\",\"ER_L4\",\"ER_L5\",\"Excess_returns\"],inplace=True)\n",
    "\n",
    "# replace nan by 0 for sentiment columns\n",
    "train_set=train_set.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test data too\n",
    "# drop rows that contain a missing value for returns\n",
    "test_set_pre_corona.dropna(subset=[\"ER_L1\",\"ER_L2\",\"ER_L3\",\"ER_L4\",\"ER_L5\",\"Excess_returns\"],inplace=True)\n",
    "test_set_pre_corona=test_set_pre_corona.fillna(0)\n",
    "\n",
    "# same for corona testset\n",
    "test_set_corona.dropna(subset=[\"ER_L1\",\"ER_L2\",\"ER_L3\",\"ER_L4\",\"ER_L5\",\"Excess_returns\"],inplace=True)\n",
    "test_set_corona=test_set_corona.fillna(0)\n",
    "\n",
    "# same for full testset\n",
    "test_set_full.dropna(subset=[\"ER_L1\",\"ER_L2\",\"ER_L3\",\"ER_L4\",\"ER_L5\",\"Excess_returns\"],inplace=True)\n",
    "test_set_full=test_set_full.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x.January.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excess returns regression multiply y with 100 to multiply the coefficients with 100\n",
    "# display impact by 100 (as Ahmad) / same effect as values as %\n",
    "y=train_set[\"Excess_returns\"]*100\n",
    "x=train_set.drop(['Excess_returns',sent_choice_pos], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.iloc[:,5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standard Regression\n",
    "# take simplest approach if there is no difference\n",
    "model_ER_plain=sm.OLS(y, x).fit()\n",
    "\n",
    "# Print the model's summary\n",
    "print(model_ER_plain.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save company coefficients to excel\n",
    "df_dummy_coef = pd.concat((model_ER_plain.params, model_ER_plain.tvalues), axis=1).iloc[11:,:]\n",
    "df_dummy_coef.index=df_dummy_coef.index.str[8:]\n",
    "df_dummy_coef=df_dummy_coef.sort_values(by=1)\n",
    "df_dummy_coef.rename(columns={0: 'beta', 1: 't'}).to_excel('ER_reg_Summary_results.xls', 'sheet1')\n",
    "df_dummy_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load decils\n",
    "df_decils=pd.read_excel(\"RIC and News Quantiles.xlsx\",index_col=\"Unnamed: 0\")\n",
    "df_analyse=df_dummy_coef.merge(df_decils, how=\"left\", left_index=True, right_on=\"Company\")\n",
    "df_analyse.rename(columns={0:\"Coeff\", 1:\"t-stat\"}, inplace=True)\n",
    "df_analyse.dropna(inplace=True)\n",
    "df_analyse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_analyse.News_Quantile.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "df_res=pd.DataFrame()\n",
    "\n",
    "for c1 in np.sort(df_analyse.News_Quantile.unique()):\n",
    "    for c2 in np.sort(df_analyse.News_Quantile.unique()):\n",
    "        t_val, p_val = stats.ttest_ind(df_analyse[df_analyse.News_Quantile==c1][\"Coeff\"], df_analyse[df_analyse.News_Quantile==c2][\"Coeff\"])\n",
    "        df_res.loc[c1, c2] = p_val\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_excel(\"1_Results/Table of p_vals by media coverage.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_industry=pd.read_excel(\"RIC and Industry Sector 2.xlsx\")\n",
    "df_analyse=df_analyse.merge(df_industry,how=\"left\",left_on=\"Company\",right_on=\"RIC\")\n",
    "df_analyse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_cross=pd.pivot_table(df_analyse, values=['Company'], index=['ICB Sector'],columns=['News_Quantile'],\\\n",
    "               aggfunc=\"count\")\n",
    "table_cross.to_excel(\"Counts industries by coverage.xlsx\")\n",
    "table_cross\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res=pd.DataFrame()\n",
    "\n",
    "for a in np.sort(df_analyse[\"ICB Sector\"].unique()):\n",
    "    for b in np.sort(df_analyse[\"ICB Sector\"].unique()):\n",
    "        t_val, p_val = stats.ttest_ind(df_analyse[df_analyse[\"ICB Sector\"]==a][\"Coeff\"], df_analyse[df_analyse[\"ICB Sector\"]==b][\"Coeff\"])\n",
    "        df_res.loc[a, b] = p_val\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_excel(\"1_Results/Table of p_vals by industry.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report t-stats\n",
    "df_res=pd.DataFrame()\n",
    "\n",
    "for a in np.sort(df_analyse[\"ICB Sector\"].unique()):\n",
    "    for b in np.sort(df_analyse[\"ICB Sector\"].unique()):\n",
    "        t_val, p_val = stats.ttest_ind(df_analyse[df_analyse[\"ICB Sector\"]==a][\"Coeff\"], df_analyse[df_analyse[\"ICB Sector\"]==b][\"Coeff\"])\n",
    "        df_res.loc[a, b] = t_val\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyse=df_analyse.groupby(\"News_Quantile\").agg(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyse.to_excel(\"Coefficients by media coverage.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conduct Regression analysis with Newey and West Standard errors \n",
    "#- robust to heteroskedasticity and autocorrelation up to five lags\n",
    "#model_ER = sm.OLS(y, x).fit()\n",
    "model_ER=sm.OLS(y, x).fit(cov_type='nw-panel',cov_kwds={'maxlags':5,'time':train_set.index})\n",
    "\n",
    "# Print the model's summary\n",
    "print(model_ER.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(model_ER.summary2().as_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ER.summary(fit)$coefficients[,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produces same results\n",
    "#model_ER2=model_ER.get_robustcov_results(cov_type='hac-panel',time=train_set.index,maxlags=5)\n",
    "#print(model_ER2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conduct Regression analysis with Huber-White std errors \n",
    "#- robust to heteroskedasticity\n",
    "model_ER_HW=sm.OLS(y, x).fit(cov_type='HC0')\n",
    "\n",
    "# Print the model's summary\n",
    "print(model_ER_HW.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Dynamic Panel Regression for News Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment regression\n",
    "y_sent=train_set[sent_choice_pos]*100\n",
    "x_sent=train_set.drop(['Excess_returns',sent_choice_pos], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct Regression analysis \n",
    "#model_sent = sm.OLS(y_sent, x_sent).fit()\n",
    "model_sent_plain=sm.OLS(y_sent, x_sent).fit()\n",
    "\n",
    "# Print the model's summary\n",
    "print(model_sent_plain.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conduct Regression analysis \n",
    "#model_sent = sm.OLS(y_sent, x_sent).fit()\n",
    "model_sent=sm.OLS(y_sent, x_sent).fit(cov_type='nw-panel',cov_kwds={'maxlags':5,'time':train_set.index})\n",
    "\n",
    "# Print the model's summary\n",
    "print(model_sent.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conduct Regression analysis \n",
    "#model_sent = sm.OLS(y_sent, x_sent).fit()\n",
    "model_sent_HW=sm.OLS(y_sent, x_sent).fit(cov_type='HC0')\n",
    "\n",
    "# Print the model's summary\n",
    "print(model_sent_HW.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://github.com/mwburke/stargazer/blob/master/examples.ipynb\n",
    "from stargazer.stargazer import Stargazer, LineLocation\n",
    "\n",
    "stargazer = Stargazer([model_ER_plain, model_sent_plain,model_ER,model_sent,model_ER_HW,model_sent_HW])\n",
    "stargazer.title(sent_choice_pos)\n",
    "stargazer.custom_columns(['Excess returns', 'Sentiment',\\\n",
    "                          'NW: Excess returns', 'NW: Sentiment',\\\n",
    "                          'HW: Excess returns', 'HW: Sentiment'], [1, 1,1,1,1,1])\n",
    "stargazer.show_model_numbers(False)\n",
    "#stargazer.significant_digits(2)\n",
    "list_relevant=['ER_L1', 'ER_L2', 'ER_L3', 'ER_L4', 'ER_L5',sent_choice_pos+'L_1',sent_choice_pos+'L_2',\\\n",
    "              sent_choice_pos+'L_3',sent_choice_pos+'L_4',sent_choice_pos+'L_5',\"Monday\"]\n",
    "stargazer.covariate_order(list_relevant)\n",
    "\n",
    "#stargazer.rename_covariates({'Age': 'Oldness'})\n",
    "\n",
    "stargazer.show_degrees_of_freedom(False)\n",
    "#stargazer.add_custom_notes(['First note', 'Second note'])\n",
    "stargazer.add_line('Company dummies', ['Yes', 'Yes','Yes','Yes','Yes','Yes'])\n",
    "#stargazer.add_line('Preferred', ['No', 'Yes'], LineLocation.FOOTER_TOP)\n",
    "\n",
    "#stargazer.significance_levels([0.1, 0.05, 0.07])\n",
    "#stargazer.append_notes(False)\n",
    "\n",
    "stargazer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the table data to a Pandas dataframe \n",
    "table = pd.read_html(stargazer.render_html())\n",
    "table=table[0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table=pd.DataFrame(table)\n",
    "df_table.to_excel(\"1_Results/Train_Regression results_\"+sent_choice_pos+\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Refit Excess returns regression with regular coefficient\n",
    "y=train_set[\"Excess_returns\"]\n",
    "x=train_set.drop(['Excess_returns',sent_choice_pos], axis=1)\n",
    "\n",
    "# Standard Regression\n",
    "# take simplest approach if there is no difference\n",
    "model_ER_plain=sm.OLS(y, x).fit()\n",
    "\n",
    "# Print the model's summary\n",
    "print(model_ER_plain.summary())\n",
    "\n",
    "# Sentiment regression\n",
    "y_sent=train_set[sent_choice_pos]\n",
    "x_sent=train_set.drop(['Excess_returns',sent_choice_pos], axis=1)\n",
    "\n",
    "\n",
    "# Conduct Regression analysis \n",
    "#model_sent = sm.OLS(y_sent, x_sent).fit()\n",
    "model_sent_plain=sm.OLS(y_sent, x_sent).fit()\n",
    "\n",
    "# Print the model's summary\n",
    "print(model_sent_plain.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate the model on test data\n",
    "\n",
    "The procedure to forecast returns only for the next day, retrain the model and forecast again takes too long.\n",
    "Thus, we decided to conduct the out-of-sample testing with the basic model only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy measures"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_mda(y, yhat):\n",
    "    \"\"\"Mean Directional Accuracy, as per:\n",
    "    https://www.wikiwand.com/en/Mean_Directional_Accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove rows that show difference between last day of company A and first day of company B\n",
    "    for i in [y,yhat]:\n",
    "        i=i.diff()\n",
    "        \n",
    "        # create Df and reset index\n",
    "        i=pd.DataFrame(i)\n",
    "        i=i.reset_index()\n",
    "        \n",
    "        #rename date column and convert to datetime\n",
    "        i.rename(columns={ i.columns[0]: \"Date\" }, inplace = True)\n",
    "        i.Date=pd.to_datetime(i.Date)\n",
    "        \n",
    "        # calculate the difference between consecutive days\n",
    "        i[\"Date_Flag\"]=i.Date.diff()\n",
    "        #i[\"Date_Flag\"]=(i[\"Date_Flag\"])\n",
    "        \n",
    "        # select only the days for which the diff. is larger then 0\n",
    "        i=i[i.Date_Flag.dt.days>0]\n",
    "        \n",
    "        # set date back to index\n",
    "        i=i.set_index(i.Date)\n",
    "        \n",
    "        # drop the date and the date flag columns\n",
    "        i=i.drop(columns=[\"Date_Flag\",\"Date\"],axis=1)\n",
    "\n",
    "    a = np.sign(y)\n",
    "    b = np.sign(yhat)\n",
    "    \n",
    "    #a = np.sign(np.diff(y))\n",
    "    #b = np.sign(np.diff(yhat))\n",
    "    \n",
    "    return np.sum(a == b)/a.shape[0]\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mda_orig(y, yhat):\n",
    "    \"\"\"Mean Directional Accuracy, as per:\n",
    "    https://www.wikiwand.com/en/Mean_Directional_Accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    a = np.sign(np.diff(y))\n",
    "    b = np.sign(np.diff(yhat))\n",
    "    \n",
    "    return np.sum(a == b)/a.shape[0]\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# define mda for returns\n",
    "def get_mda_price(y, yhat):\n",
    "    \"\"\"Mean Directional Accuracy, as per:\n",
    "    https://www.wikiwand.com/en/Mean_Directional_Accuracy\n",
    "    \"\"\"\n",
    "    a = np.sign((y))\n",
    "    b = np.sign((yhat))\n",
    "    return np.sum(a == b)/a.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline**\n",
    "\n",
    "Pre Corona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_pre_corona = test_set_pre_corona.reset_index()\n",
    "test_set_corona = test_set_corona.reset_index()\n",
    "test_set_pre_corona.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# use mean return to create baseline for each company\n",
    "\n",
    "# Set Company and date as multiindex\n",
    "test_set_pre_corona = test_set_pre_corona.reset_index()\n",
    "test_set_pre_corona = test_set_pre_corona.set_index(['Companies', 'Date'])\n",
    "\n",
    "# create a new column containing the mean per company\n",
    "test_set_pre_corona[\"mean_return\"]=test_set_pre_corona.groupby(level=0)[\"Excess_returns\"].transform('mean')\n",
    "test_set_pre_corona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mean return to create baseline for each company\n",
    "train_set=df_main[df_main.index < \"2019-10-01\"]\n",
    "# calculate mean by company\n",
    "comp_mean=train_set.groupby('Companies')[\"Excess_returns\"].agg('mean')\n",
    "comp_mean=comp_mean.reset_index().rename(columns={\"Excess_returns\": \"mean_return\"})\n",
    "\n",
    "comp_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge returns on pre corona testset and drop all companies that were non-stationary  by using \"inner\"\n",
    "test_set_pre_corona=test_set_pre_corona.merge(comp_mean, how=\"inner\", left_on=\"Companies\",right_on=\"Companies\")\n",
    "\n",
    "# merge returns on  corona testset and drop all companies that were non-stationary  by using \"inner\"\n",
    "test_set_corona=test_set_corona.merge(comp_mean, how=\"inner\", left_on=\"Companies\",right_on=\"Companies\")\n",
    "\n",
    "# merge returns on  full testset and drop all companies that were non-stationary  by using \"inner\"\n",
    "test_set_full=test_set_full.merge(comp_mean, how=\"inner\", left_on=\"Companies\",right_on=\"Companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_pre_corona.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_corona.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double-check calculation\n",
    "#test_set_pre_corona.loc[\"A\"][\"Excess_returns\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find predom sign in training set\n",
    "np.sign(train_set.Excess_returns).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As np.sign() returns -1 / 1 indicating the sign, from the summing the fields value, we can obtain the predominant sign. As the calculated sum is positive, we can conclude that the predom. sign is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show only positive\n",
    "np.sign(train_set.Excess_returns)[np.sign(train_set.Excess_returns)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show only negative\n",
    "np.sign(train_set.Excess_returns)[np.sign(train_set.Excess_returns)<0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_set[\"return_sign\"]=np.sign(train_set[\"Excess_returns\"])\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# find predominant sign for each company\n",
    "train_set[\"sign_diff\"]=train_set.groupby(\"Companies\")[\"return_sign\"].transform(\"sum\")\n",
    "train_set.head()\n",
    "\n",
    "#test_set_pre_corona[\"pred_sign\"]=np.sign(test_set_pre_corona[\"sign_diff\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check computation\n",
    "#test_set_pre_corona.loc[\"A\"][\"return_sign\"].sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# use mean return to create baseline for each company\n",
    "\n",
    "# Set Company and date as multiindex\n",
    "test_set_corona=test_set_corona.reset_index()\n",
    "test_set_corona = test_set_corona.set_index(['Companies', 'Date'])\n",
    "\n",
    "# create a new column containing the mean per company\n",
    "test_set_corona[\"mean_return\"]=test_set_corona.groupby(level=0)[\"Excess_returns\"].transform('mean')\n",
    "\n",
    "test_set_corona[\"return_sign\"]=np.sign(test_set_corona[\"Excess_returns\"])\n",
    "\n",
    "# find predominant sign for each company\n",
    "test_set_corona[\"sign_diff\"]=test_set_corona.groupby(level=0)[\"return_sign\"].transform(\"sum\")\n",
    "\n",
    "# drop rows with missing values\n",
    "test_set_pre_corona.dropna(inplace=True)\n",
    "test_set_corona.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_pre_corona[\"Excess_returns\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate RMSE / MDA per company and average score for the full test set by industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join industries to trainingset\n",
    "df_industry=pd.read_excel(\"RIC and Industry Sector 2.xlsx\")\n",
    "df_industry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join data on test set\n",
    "test_set_full=test_set_full.merge(df_industry,how=\"left\",left_on=\"Companies\",right_on=\"RIC\")\n",
    "test_set_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_full.Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_full.drop_duplicates(subset=\"Companies\")[\"ICB Sector\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_full[test_set_full[\"ICB Sector\"].isnull()][\"Companies\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_industry_results=pd.DataFrame()\n",
    "df_industry_results[\"Industry\"]=test_set_full[\"ICB Sector\"].unique()\n",
    "df_industry_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return MDA\n",
    "\n",
    "# mean baseline\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"ICB Sector\"].unique()[:]:\n",
    "    x=get_mda_orig(test_set_full[test_set_full[\"ICB Sector\"]==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full[\"ICB Sector\"]==i][\"mean_return\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "df_industry_results[\"MDA_mean\"]=list_scores\n",
    "#df_industry_results.head()\n",
    "\n",
    "# persistence baseline\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"ICB Sector\"].unique()[:]:\n",
    "    x=get_mda_orig(test_set_full[test_set_full[\"ICB Sector\"]==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full[\"ICB Sector\"]==i][\"ER_L1\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "df_industry_results[\"MDA_pers\"]=list_scores\n",
    "#df_industry_results.head()\n",
    "\n",
    "\n",
    "# positive baseline\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"ICB Sector\"].unique()[:]:\n",
    "    x=get_mda_orig(test_set_full[test_set_full[\"ICB Sector\"]==i][\"Excess_returns\"],\\\n",
    "                    np.arange(test_set_full[test_set_full[\"ICB Sector\"]==i][\"mean_return\"].shape[0]))\n",
    "    list_scores.append(x)\n",
    "    \n",
    "df_industry_results[\"MDA_pos\"]=list_scores\n",
    "df_industry_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE \n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"ICB Sector\"].unique()[:]:\n",
    "    x=np.sqrt(mean_squared_error(test_set_full[test_set_full[\"ICB Sector\"]==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full[\"ICB Sector\"]==i][\"mean_return\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "df_industry_results[\"RMSE_mean\"]=list_scores\n",
    "\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"ICB Sector\"].unique()[:]:\n",
    "    x= np.sqrt(mean_squared_error(test_set_full[test_set_full[\"ICB Sector\"]==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full[\"ICB Sector\"]==i][\"ER_L1\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "df_industry_results[\"RMSE_pers\"]=list_scores\n",
    "df_industry_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dependent and independent variables\n",
    "x_test_full=test_set_full.drop(['Excess_returns',\\\n",
    "                                       sent_choice_pos,\"mean_return\",\"RIC\",\"ICB Sector\",'Companies'], axis=1)\n",
    "\n",
    "y_test_full=test_set_full[\"Excess_returns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all values straight\n",
    "y_hat_full = model_ER_plain.predict(x_test_full)\n",
    "y_hat_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include in orgi datafram\n",
    "test_set_full[\"y_hat\"]=y_hat_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDA\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"ICB Sector\"].unique()[:]:\n",
    "    x=get_mda_orig(test_set_full[test_set_full[\"ICB Sector\"]==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full[\"ICB Sector\"]==i][\"y_hat\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "df_industry_results[\"MDA_dynp\"]=list_scores\n",
    "#df_industry_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE \n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"ICB Sector\"].unique()[:]:\n",
    "    x=np.sqrt(mean_squared_error(test_set_full[test_set_full[\"ICB Sector\"]==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full[\"ICB Sector\"]==i][\"y_hat\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "df_industry_results[\"RMSE_dynp\"]=list_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_industry_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_industry_results.to_excel(\"1_Results/OOS_Industry performance.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate RMSE / MDA per company and average score for the full test set by media coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join industries to trainingset\n",
    "df_decils=pd.read_excel(\"RIC and News Quantiles.xlsx\",index_col=\"Unnamed: 0\")\n",
    "df_decils.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join data on test set\n",
    "test_set_full=test_set_full.merge(df_decils, how=\"left\", left_on=\"Companies\", right_on=\"Company\")\n",
    "test_set_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set_full[test_set_full[\"News_Quantile\"].isnull()][\"Companies\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_full.drop_duplicates(subset=\"Company\")[\"News_Quantile\"].value_counts()\n",
    "test_set_full.dropna(subset=[\"News_Quantile\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decils_results=pd.DataFrame()\n",
    "df_decils_results[\"News_Quantile\"]=test_set_full[\"News_Quantile\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return MDA\n",
    "\n",
    "# mean baseline\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"News_Quantile\"].unique()[:]:\n",
    "    x=get_mda_orig(test_set_full[test_set_full[\"News_Quantile\"]==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full[\"News_Quantile\"]==i][\"mean_return\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "df_decils_results[\"MDA_mean\"]=list_scores\n",
    "#df_industry_results.head()\n",
    "\n",
    "# persistence baseline\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"News_Quantile\"].unique()[:]:\n",
    "    x=get_mda_orig(test_set_full[test_set_full[\"News_Quantile\"]==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full[\"News_Quantile\"]==i][\"ER_L1\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "df_decils_results[\"MDA_pers\"]=list_scores\n",
    "#df_industry_results.head()\n",
    "\n",
    "\n",
    "# positive baseline\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"News_Quantile\"].unique()[:]:\n",
    "    x=get_mda_orig(test_set_full[test_set_full[\"News_Quantile\"]==i][\"Excess_returns\"],\\\n",
    "                    np.arange(test_set_full[test_set_full[\"News_Quantile\"]==i][\"mean_return\"].shape[0]))\n",
    "    list_scores.append(x)\n",
    "    \n",
    "df_decils_results[\"MDA_pos\"]=list_scores\n",
    "df_decils_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_full[\"News_Quantile\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE \n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"News_Quantile\"].unique()[:]:\n",
    "    x=np.sqrt(mean_squared_error(test_set_full[test_set_full[\"News_Quantile\"]==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full[\"News_Quantile\"]==i][\"mean_return\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "df_decils_results[\"RMSE_mean\"]=list_scores\n",
    "\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"News_Quantile\"].unique()[:]:\n",
    "    x= np.sqrt(mean_squared_error(test_set_full[test_set_full[\"News_Quantile\"]==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full[\"News_Quantile\"]==i][\"ER_L1\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "df_decils_results[\"RMSE_pers\"]=list_scores\n",
    "df_decils_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dependent and independent variables\n",
    "x_test_full=test_set_full.drop(['Excess_returns',\\\n",
    "                                       sent_choice_pos,\"mean_return\",\"RIC\",\"ICB Sector\",'Companies',\"Company\",\"News_Quantile\",\"y_hat\"], axis=1)\n",
    "\n",
    "y_test_full=test_set_full[\"Excess_returns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all values straight\n",
    "y_hat_full = model_ER_plain.predict(x_test_full)\n",
    "y_hat_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include in orginal dataframe\n",
    "test_set_full[\"y_hat\"]=y_hat_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDA\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"News_Quantile\"].unique()[:]:\n",
    "    x=get_mda_orig(test_set_full[test_set_full[\"News_Quantile\"]==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full[\"News_Quantile\"]==i][\"y_hat\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "df_decils_results[\"MDA_dynp\"]=list_scores\n",
    "#df_industry_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE \n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full[\"News_Quantile\"].unique()[:]:\n",
    "    x=np.sqrt(mean_squared_error(test_set_full[test_set_full[\"News_Quantile\"]==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full[\"News_Quantile\"]==i][\"y_hat\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "df_decils_results[\"RMSE_dynp\"]=list_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decils_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decils_results.to_excel(\"1_Results/OOS_News coverage performance.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate RMSE / MDA per company and average score for the full test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Return MDA\n",
    "\n",
    "# mean baseline\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full.Companies.unique()[:]:\n",
    "    x=get_mda_orig(test_set_full[test_set_full.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full.Companies==i][\"mean_return\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "mean_bl_mda=np.mean(list_scores)\n",
    "print(mean_bl_mda)\n",
    "\n",
    "# persistence baseline\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full.Companies.unique()[:]:\n",
    "    x=get_mda_orig(test_set_full[test_set_full.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full.Companies==i][\"ER_L1\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "per_bl_mda=np.mean(list_scores)\n",
    "print(per_bl_mda)\n",
    "\n",
    "# positive baseline\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full.Companies.unique()[:]:\n",
    "    x=get_mda_orig(test_set_full[test_set_full.Companies==i][\"Excess_returns\"],\\\n",
    "                    np.arange(test_set_full[test_set_full.Companies==i][\"mean_return\"].shape[0]))\n",
    "    list_scores.append(x)\n",
    "    \n",
    "up_bl_mda=np.mean(list_scores)\n",
    "\n",
    "#return mean score per company\n",
    "print(up_bl_mda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE \n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full.Companies.unique()[:]:\n",
    "    x=np.sqrt(mean_squared_error(test_set_full[test_set_full.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full.Companies==i][\"mean_return\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "mean_bl_rmse=np.mean(list_scores)\n",
    "print(mean_bl_rmse)\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_full.Companies.unique()[:]:\n",
    "    x= np.sqrt(mean_squared_error(test_set_full[test_set_full.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_full[test_set_full.Companies==i][\"ER_L1\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "per_bl_rmse=np.mean(list_scores)\n",
    "print(per_bl_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate RMSE / MDA per company and average score by Corona split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return MDA\n",
    "\n",
    "# Pre Corona\n",
    "# mean baseline\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_pre_corona.Companies.unique()[:]:\n",
    "    x=get_mda_orig(test_set_pre_corona[test_set_pre_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_pre_corona[test_set_pre_corona.Companies==i][\"mean_return\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "pc_mean_bl_mda=np.mean(list_scores)\n",
    "\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_pre_corona.Companies.unique()[:]:\n",
    "    x=get_mda_orig(test_set_pre_corona[test_set_pre_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_pre_corona[test_set_pre_corona.Companies==i][\"ER_L1\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "pc_per_bl_mda=np.mean(list_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_per_bl_mda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE \n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_pre_corona.Companies.unique()[:]:\n",
    "    x=np.sqrt(mean_squared_error(test_set_pre_corona[test_set_pre_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_pre_corona[test_set_pre_corona.Companies==i][\"mean_return\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "pc_mean_bl_rmse=np.mean(list_scores)\n",
    "\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_pre_corona.Companies.unique()[:]:\n",
    "    x= np.sqrt(mean_squared_error(test_set_pre_corona[test_set_pre_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_pre_corona[test_set_pre_corona.Companies==i][\"ER_L1\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "pc_per_bl_rmse=np.mean(list_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_mean_bl_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return RMSE and MDA\n",
    "\n",
    "# Pre Corona\n",
    "# mean baseline\n",
    "#pc_mean_bl_rmse = np.sqrt(mean_squared_error(test_set_pre_corona[\"Excess_returns\"],test_set_pre_corona[\"mean_return\"]))\n",
    "#pc_mean_bl_mda = get_mda(test_set_pre_corona[\"Excess_returns\"],test_set_pre_corona[\"mean_return\"])\n",
    "print(f\"Pre corona Mean baseline\")\n",
    "print(f\"RMSE: {pc_mean_bl_rmse}\")\n",
    "print(f\"MDA: {pc_mean_bl_mda}\")\n",
    "\n",
    "# persistence baseline\n",
    "#pc_per_bl_rmse = np.sqrt(mean_squared_error(test_set_pre_corona[\"Excess_returns\"],test_set_pre_corona[\"ER_L1\"]))\n",
    "#pc_per_bl_mda = get_mda(test_set_pre_corona[\"Excess_returns\"],test_set_pre_corona[\"ER_L1\"])\n",
    "print(f\"Pre corona persistence baseline\")\n",
    "print(f\"RMSE: {pc_per_bl_rmse}\")\n",
    "print(f\"MDA: {pc_per_bl_mda}\")\n",
    "\n",
    "# predominant sign baseline    \n",
    "#pc_ds_bl_mda = get_mda(test_set_pre_corona[\"Excess_returns\"],test_set_pre_corona[\"sign_diff\"])\n",
    "#print(f\"Pre corona predom. sign baseline\")\n",
    "#print(f\"MDA: {pc_ds_bl_mda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if computation is correct pc_mean_bl_mda\n",
    "#test_set_pre_corona[[\"Excess_returns\",\"mean_return\",\"ER_L1\"]].to_excel(\"MDA Performance analysis.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return MDA for always positive baseline\n",
    "\n",
    "# Pre Corona\n",
    "# positive baseline\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_pre_corona.Companies.unique()[:]:\n",
    "    x=get_mda_orig(test_set_pre_corona[test_set_pre_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                  # np.random.randint(1, 99999999999999,\\\n",
    "                           #          test_set_pre_corona[test_set_pre_corona.Companies==i][\"mean_return\"].shape[0]))\n",
    "                    np.arange(test_set_pre_corona[test_set_pre_corona.Companies==i][\"mean_return\"].shape[0]))\n",
    "    list_scores.append(x)\n",
    "    \n",
    "pc_up_bl_mda=np.mean(list_scores)\n",
    "#return mean score per company\n",
    "print(\"pre corona\")\n",
    "print(np.mean(list_scores))\n",
    "\n",
    "# Post Corona\n",
    "# positive baseline\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_corona.Companies.unique()[:]:\n",
    "    x=get_mda_orig(test_set_corona[test_set_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                   #np.random.randint(1, 99999999999999,\\\n",
    "                    #                 test_set_corona[test_set_corona.Companies==i][\"mean_return\"].shape[0]))\n",
    "                   np.arange(test_set_corona[test_set_corona.Companies==i][\"mean_return\"].shape[0]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "dc_up_bl_mda=np.mean(list_scores)\n",
    "    \n",
    "#return mean score per company\n",
    "print(\"corona\")\n",
    "print(np.mean(list_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return MDA\n",
    "\n",
    "# Pre Corona\n",
    "# mean baseline\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_corona.Companies.unique()[:]:\n",
    "    x=get_mda_orig(test_set_corona[test_set_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_corona[test_set_corona.Companies==i][\"mean_return\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "dc_mean_bl_mda=np.mean(list_scores)\n",
    "\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_corona.Companies.unique()[:]:\n",
    "    x=get_mda_orig(test_set_corona[test_set_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_corona[test_set_corona.Companies==i][\"ER_L1\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "dc_per_bl_mda=np.mean(list_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE \n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_corona.Companies.unique()[:]:\n",
    "    x=np.sqrt(mean_squared_error(test_set_corona[test_set_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_corona[test_set_corona.Companies==i][\"mean_return\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "dc_mean_bl_rmse=np.mean(list_scores)\n",
    "\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_corona.Companies.unique()[:]:\n",
    "    x= np.sqrt(mean_squared_error(test_set_corona[test_set_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_corona[test_set_corona.Companies==i][\"ER_L1\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "dc_per_bl_rmse=np.mean(list_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post Corona\n",
    "# mean baseline\n",
    "#dc_mean_bl_rmse = np.sqrt(mean_squared_error(test_set_corona[\"Excess_returns\"],test_set_corona[\"mean_return\"]))\n",
    "#dc_mean_bl_mda = get_mda(test_set_corona[\"Excess_returns\"],test_set_corona[\"mean_return\"])\n",
    "print(f\"Corona Mean baseline\")\n",
    "print(f\"RMSE: {dc_mean_bl_rmse}\")\n",
    "print(f\"MDA: {dc_mean_bl_mda}\")\n",
    "\n",
    "# persistence baseline\n",
    "#dc_per_bl_rmse = np.sqrt(mean_squared_error(test_set_corona[\"Excess_returns\"],test_set_corona[\"ER_L1\"]))\n",
    "#dc_per_bl_mda = get_mda(test_set_corona[\"Excess_returns\"],test_set_corona[\"ER_L1\"])\n",
    "print(f\"Corona persistence baseline\")\n",
    "print(f\"RMSE: {dc_per_bl_rmse}\")\n",
    "print(f\"MDA: {dc_per_bl_mda}\")\n",
    "\n",
    "# predominant sign baseline    \n",
    "#dc_ds_bl_mda = get_mda(test_set_corona[\"Excess_returns\"],test_set_corona[\"sign_diff\"])\n",
    "#print(f\"Pre corona predom. sign baseline\")\n",
    "#print(f\"MDA: {dc_ds_bl_mda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_pre_corona.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove companies, reset index\n",
    "#test_set_pre_corona = test_set_pre_corona.reset_index()\n",
    "test_set_pre_corona = test_set_pre_corona.set_index(['Date'])\n",
    "\n",
    "#test_set_corona = test_set_corona.reset_index()\n",
    "test_set_corona = test_set_corona.set_index(['Date'])\n",
    "\n",
    "# drop companies from both datasets\n",
    "#test_set_pre_corona = test_set_pre_corona.drop(['Companies'],axis=1)\n",
    "#test_set_corona = test_set_corona.drop(['Companies'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use model to make predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dependent and independent variables\n",
    "x_test_pre_c=test_set_pre_corona.drop(['Excess_returns',\\\n",
    "                                       sent_choice_pos,\"mean_return\",'Companies'], axis=1)\n",
    "\n",
    "x_test_c=test_set_corona.drop(['Excess_returns',\\\n",
    "                                       sent_choice_pos,\"mean_return\",'Companies'], axis=1)\n",
    "\n",
    "y_test_pre_c=test_set_pre_corona[\"Excess_returns\"]\n",
    "y_test_c=test_set_corona[\"Excess_returns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pre_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all values straight\n",
    "y_hat_pre_c = model_ER_plain.predict(x_test_pre_c)\n",
    "y_hat_pre_c\n",
    "\n",
    "y_hat_c = model_ER_plain.predict(x_test_c)\n",
    "y_hat_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include estimates in orig dataframe\n",
    "test_set_pre_corona[\"y_hat\"]=y_hat_pre_c\n",
    "test_set_corona[\"y_hat\"]=y_hat_c\n",
    "test_set_corona.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pre_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_pre_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return RMSE and MDA\n",
    "#pc_var_rmse = np.sqrt(mean_squared_error(y_test_pre_c,y_hat_pre_c))\n",
    "print(f\"Pre Corona:\")\n",
    "\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_pre_corona.Companies.unique()[:]:\n",
    "    x= np.sqrt(mean_squared_error(test_set_pre_corona[test_set_pre_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_pre_corona[test_set_pre_corona.Companies==i][\"y_hat\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "pc_var_rmse=np.mean(list_scores)\n",
    "\n",
    "print(f\"RMSE: {pc_var_rmse}\")\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_pre_corona.Companies.unique()[:]:\n",
    "    x=get_mda_orig(test_set_pre_corona[test_set_pre_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_pre_corona[test_set_pre_corona.Companies==i][\"y_hat\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "pc_var_mda=np.mean(list_scores)\n",
    "\n",
    "#pc_var_mda = get_mda(y_test_pre_c,y_hat_pre_c)\n",
    "print(f\"MDA: {pc_var_mda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pre corona scores dist\n",
    "df_analyse=pd.DataFrame(list_scores)\n",
    "df_analyse[\"Company\"]=test_set_pre_corona.Companies.unique()\n",
    "df_analyse.sort_values(by=0, ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load decils\n",
    "df_decils=pd.read_excel(\"RIC and News Quantiles.xlsx\",index_col=\"Unnamed: 0\")\n",
    "df_analyse=df_analyse.merge(df_decils, how=\"left\", left_on=\"Company\", right_on=\"Company\")\n",
    "df_analyse.rename(columns={0:\"MDA_score\"}, inplace=True)\n",
    "df_analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyse.MDA_score.hist(by=df_analyse[\"News_Quantile\"], bins=10, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyse.groupby(\"News_Quantile\").agg((\"mean\",\"median\",\"min\",\"max\",\"std\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return RMSE and MDA\n",
    "#dc_var_rmse = np.sqrt(mean_squared_error(y_hat_c, y_test_c))\n",
    "print(f\"During Corona:\")\n",
    "\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_corona.Companies.unique()[:]:\n",
    "    x= np.sqrt(mean_squared_error(test_set_corona[test_set_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_corona[test_set_corona.Companies==i][\"y_hat\"]))\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "dc_var_rmse=np.mean(list_scores)\n",
    "\n",
    "print(f\"RMSE: {dc_var_rmse}\")\n",
    "\n",
    "list_scores=[]\n",
    "for i in test_set_corona.Companies.unique()[:]:\n",
    "    x=get_mda_orig(test_set_corona[test_set_corona.Companies==i][\"Excess_returns\"],\\\n",
    "                   test_set_corona[test_set_corona.Companies==i][\"y_hat\"])\n",
    "    list_scores.append(x)\n",
    "\n",
    "#return mean score per company\n",
    "dc_var_mda=np.mean(list_scores)\n",
    "\n",
    "#dc_var_mda = get_mda(y_test_c,y_hat_c)\n",
    "print(f\"MDA: {dc_var_mda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "### RMSE  results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_table = pd.DataFrame({\n",
    "    'method': ['Mean baseline', 'Persistence baseline','Dynamic Panel'],\n",
    "    'Pre Corona': [pc_mean_bl_rmse,pc_per_bl_rmse,pc_var_rmse],\n",
    "    'During Corona': [dc_mean_bl_rmse,dc_per_bl_rmse,dc_var_rmse],\n",
    "})\n",
    "\n",
    "rmse_table.set_index(\"method\", inplace=True)\n",
    "\n",
    "# add columns with percent changes on the baselines\n",
    "rmse_table['Pre Corona -  % Change on Mean baseline'] = (rmse_table['Pre Corona'] / pc_mean_bl_rmse -1)\n",
    "rmse_table['Pre Corona -  % Change on Persistence baseline'] = (rmse_table['Pre Corona'] / pc_per_bl_rmse -1)\n",
    "rmse_table['During Corona - % Change on Mean baseline'] =  (rmse_table['During Corona'] / dc_mean_bl_rmse -1)\n",
    "rmse_table['During Corona - % Change on Persistence baseline'] =  (rmse_table['During Corona'] / dc_per_bl_rmse -1)\n",
    "\n",
    "rmse_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar performance\n",
    "\n",
    "### MDA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mda_table = pd.DataFrame({\n",
    "    'method': ['Up baseline', 'Persistence baseline','Dynamic Panel'],\n",
    "    'Pre Corona':    [pc_up_bl_mda,pc_per_bl_mda,pc_var_mda],\n",
    "    'During Corona': [dc_up_bl_mda,dc_per_bl_mda,dc_var_mda],\n",
    "})\n",
    "\n",
    "mda_table.set_index(\"method\", inplace=True)\n",
    "\n",
    "# add columns with percent changes on the baselines\n",
    "mda_table['Pre Corona -  % Change on Up baseline'] = (mda_table['Pre Corona'] - pc_up_bl_mda )\n",
    "mda_table['Pre Corona -  % Change on Persistence baseline'] = (mda_table['Pre Corona'] - pc_per_bl_mda )\n",
    "mda_table['During Corona - % Change on Up baseline'] =  (mda_table['During Corona'] - dc_up_bl_mda )\n",
    "mda_table['During Corona - % Change on Persistence baseline'] =  (mda_table['During Corona'] - dc_per_bl_mda )\n",
    "\n",
    "mda_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge increase in MDA performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_results=[rmse_table,mda_table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import ExcelWriter\n",
    "# from pandas.io.parsers import ExcelWriter\n",
    "\n",
    "def save_xls(list_dfs, xls_path):\n",
    "    with ExcelWriter(xls_path) as writer:\n",
    "        for n, df in enumerate(list_dfs):\n",
    "            df.to_excel(writer,'sheet%s' % n)\n",
    "        writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store to excel\n",
    "save_xls(list_results,\"1_Results/Test_Regression results_\"+sent_choice_pos+\".xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
